{
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "name": "",
  "signature": "sha256:96b5f0d97985d52f652f8e39cd7b0c6a972393c5e88d7cc309794345dd86f895"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Exploring Quantopian\n",
      "\n",
      "There is collaboration and [a Python IDE][ide-api] directly on the site, which is how the user interacts with their data.\n",
      "\n",
      "Both of us are brand new to the site, so we are exploring in order of the help file.\n",
      "\n",
      "[ide-api]: https://www.quantopian.com/help#ide-api"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Quick summary of the API\n",
      "\n",
      "- Symbols:\n",
      "\n",
      "        symbol('goog')  # single\n",
      "        symbols('goog', 'fb')  # multiple\n",
      "        sid(24)  # unique ID to Quantopian -- 24 is for aapl\n",
      "        \n",
      "- [Fundamentals] [fundamentals_reference]:\n",
      "    + Available for 8000 companies, with over 670 metrics\n",
      "    + Accessed using **get_fundamentals** with the same syntax as SQLAlchemy;\n",
      "      returns a pandas dataframe\n",
      "    + Not available during live trading, only in **before_trading_start**\n",
      "      (once per day) to be stored in the **context** and used in the function\n",
      "      **handle_data**\n",
      "      \n",
      "- [Ordering][ordering_reference]: market, limit, stop, stop limit\n",
      "- [Scheduling][daterules]: frequency in days, weeks, months,\n",
      "  plus order time of day in minutes\n",
      "- [Allowed modules][modules]\n",
      "- [Example algorithms][sample_basic]\n",
      "\n",
      "[fundamentals_reference]: https://www.quantopian.com/help/fundamentals\n",
      "[ordering_reference]: https://www.quantopian.com/help#ide-ordering\n",
      "[daterules]: https://www.quantopian.com/help#ide-daterules\n",
      "[modules]: https://www.quantopian.com/help#ide-module-import\n",
      "[sample_basic]: https://www.quantopian.com/help#ide-sample-basic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Actually...\n",
      "The API is so thin -- it's really just about trading -- so instead explore machine\n",
      "learning using existing data outside of the Quantopian environment. Also, they\n",
      "provide [zipline] [zipline], their backtest functions, as open-source code\n",
      "([github repo][zipline-git] / [pypi page][zipline-pypi]) to test outside\n",
      "of the quantopian environment. [Here is a how-to] [zipline-howto]\n",
      "\n",
      "[zipline]: http://www.zipline.io/#quickstart\n",
      "[zipline-git]: https://github.com/quantopian/zipline\n",
      "[zipline-pypi]: https://pypi.python.org/pypi/zipline/0.7.0\n",
      "[zipline-howto]: http://twiecki.github.io/zipline_in_the_cloud_talk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "import pandas.io.data as web\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import requests\n",
      "import datetime\n",
      "\n",
      "from database import Database\n",
      "db = Database()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#list of fortune 500 companies ticke symbols\n",
      "chicago_companies = ['ADM',  'BA',  'WGN', 'UAL', 'SHLD',\n",
      "                     'MDLZ', 'ALL', 'MCD', 'EXC', 'ABT',\n",
      "                     'ABBV', 'KRFT', 'ITW', 'NAV', 'CDW',\n",
      "                     'RRD',  'GWW',  'DFS', 'DOV', 'MSI',\n",
      "                     'TEN',  'INGR', 'NI', 'TEG', 'CF',\n",
      "                     'ORI', 'USTR', 'LKQ'\n",
      "                     ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = \"http://ichart.yahoo.com/table.csv?s={stock}&a={strt_month}&b={strt_day}&c={strt_year}&d={end_month}&e={end_day}&f={end_year}&g=w&ignore=.csv\"\n",
      "params = {\"stock\":'ADM', \"strt_month\": 1, 'strt_day':1, 'strt_year': 2010,\n",
      "          \"end_month\": 1, \"end_day\": 1, \"end_year\": 2014}\n",
      "new_url = url.format(**params)\n",
      "print url.format(**params)\n",
      "\n",
      "data_new = web.DataReader(chicago_companies, 'yahoo', datetime.datetime(2010,1,1), datetime.datetime(2014,1,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all Stock data is contained in 3D datastructure called a panel\n",
      "data_new\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ADM = pd.read_csv('data/table (4).csv')\n",
      "ADM.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ADM = ADM.sort_index(by='Date')\n",
      "\n",
      "ADM['Short'] = pd.rolling_mean(ADM['Close'], 4)\n",
      "ADM['Long'] = pd.rolling_mean(ADM['Close'], 12)\n",
      "\n",
      "ADM.plot(x='Date', y=['Close', 'Long', 'Short'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "buy_signals = np.where(pd.rolling_mean(ADM['Close'], 4) > pd.rolling_mean(ADM['Close'], 12), 1.0, 0.0)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Machine Learning\n",
      "[Scikit Learn] [sklearn]'s home page divides up the space of machine learning well,\n",
      "but the [Mahout algorithms list] [mahout-algo] has a more comprehensive list of algorithms.\n",
      "From both:\n",
      "- **Collaborative filtering**<br/>\n",
      "  'because you bought **these**, we recommend **this**'\n",
      "- **Classification**<br/>\n",
      "  'people with these characteristics, if sent a mailer, will buy something 30% of the time'\n",
      "- **Clustering**<br/>\n",
      "  'our customers naturally fall into these groups: urban singles, guys with dogs, women 25-35 who like rap'\n",
      "- **Dimension reduction**<br/>\n",
      "  'a preprocessing step before regression that can also identify the most significant contributors to variation'\n",
      "- **Topics**<br/>\n",
      "  'the posts in this user group are related to either local politics, music, or sports'\n",
      "\n",
      "\n",
      "\n",
      "The S&P 500 dataset is great for us to quickly explore regression, clustering, and principal component analysis.\n",
      "We can also backtest here using Quantopian's [zipline library] [zipline].\n",
      "\n",
      "[sklearn]: http://scikit-learn.org/stable/\n",
      "[mahout-algo]: https://mahout.apache.org/users/basics/algorithms.html\n",
      "[zipline]: http://twiecki.github.io/zipline_in_the_cloud_talk/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Regression\n",
      "We can calculate our own ['Beta'] [nasdaq_beta] by pulling the S&P 500 index values from Yahoo using Pandas and then regressing each of our components of the S&P500 with it. The NASDAQ defines Beta as the coefficient found from regressing the individual stock's returns in excess of the 90-day treasury rate on the S&P 500's returns in excess of the 90-day rate. Nasdaq cautions that Beta can change over time and that it could be different for positive and negative changes.\n",
      "\n",
      "- Look at the Chicago companies for fun (database select from our dataset)\n",
      "- Get the S&P 500 ('^GSPC') from Yahoo Finance\n",
      "- Get the 90-day treasury bill rates ('^IRX') from Yahoo Finance\n",
      "\n",
      "\n",
      "The equation for the regression will be:\n",
      "\n",
      "(Return - Treas90) = Beta * (SP500 - Treas90) + alpha\n",
      "\n",
      "\n",
      "[nasdaq_beta]: http://www.nasdaq.com/investing/glossary/b/beta"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date_range = db.select_one(\"SELECT min(dt), max(dt) from return;\", columns=(\"start\", \"end\"))\n",
      "\n",
      "chicago_companies = ['ADM',  'BA', 'MDLZ', 'ALL', 'MCD', 'EXC', 'ABT',\n",
      "                     'ABBV', 'KRFT', 'ITW', 'GWW',  'DFS', 'DOV', 'MSI',\n",
      "                     'NI', 'TEG', 'CF' ]\n",
      "chicago_returns = db.select('SELECT dt, \"{}\" FROM return ORDER BY dt;'.format(\n",
      "                             '\", \"'.join((c.lower() for c in chicago_companies))),\n",
      "                            columns=[\"Date\"] + chicago_companies)\n",
      "\n",
      "chi_dates = [row.pop(\"Date\") for row in chicago_returns]\n",
      "chicago_returns = pd.DataFrame(chicago_returns, index=chi_dates)\n",
      "\n",
      "sp500 = web.DataReader('^GSPC', 'yahoo', date_range['start'], date_range['end'])\n",
      "sp500['sp500'] = sp500['Adj Close'].diff() / sp500['Adj Close']\n",
      "\n",
      "treas90 = web.DataReader('^IRX', 'yahoo', date_range['start'], date_range['end'])\n",
      "treas90['treas90'] = treas90['Adj Close'].diff() / treas90['Adj Close']\n",
      "\n",
      "chicago_returns = chicago_returns.join(sp500['sp500'])\n",
      "chicago_returns = chicago_returns.join(treas90['treas90'])\n",
      "chicago_returns.drop(chicago_returns.head(1).index, inplace=True)\n",
      "chicago_returns = chicago_returns.sub(chicago_returns['treas90'], axis=0)\n",
      "\n",
      "chicago_returns.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "chicago_returns = chicago_returns / 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For ordinary least squares regression\n",
      "from pandas.stats.api import ols"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regressions = {}\n",
      "for y in chicago_returns.columns:\n",
      "    if y not in ('sp500', 'treas90'):\n",
      "        df = chicago_returns.dropna(subset=[y, 'sp500'])[[y, 'sp500']]\n",
      "        regressions[y] = ols(y=df[y], x=df['sp500'])\n",
      "        \n",
      "regressions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "symbols = sorted(regressions.keys())\n",
      "data = []\n",
      "for s in symbols:\n",
      "    data.append(dict(alpha=regressions[s].beta[1], beta=regressions[s].beta[0]))\n",
      "\n",
      "betas = pd.DataFrame(data=data,index=symbols)\n",
      "betas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betas.values[0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "fig.suptitle('Betas vs Alphas', fontsize=14, fontweight='bold')\n",
      "\n",
      "ax = fig.add_subplot(111)\n",
      "fig.subplots_adjust(top=0.85)\n",
      "#ax.set_title('axes title')\n",
      "\n",
      "ax.set_xlabel('Beta')\n",
      "ax.set_ylabel('Alpha')\n",
      "\n",
      "for i in range(len(betas.index)):\n",
      "    ax.text(betas.values[i,1], betas.values[i,0], betas.index[i]) #, bbox={'facecolor':'slateblue', 'alpha':0.5, 'pad':10})\n",
      "    \n",
      "ax.set_ylim([0,0.2])\n",
      "ax.set_xlim([0.75, 1.2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betas.describe()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(ax.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Clustering and Principal Components\n",
      "We can use clustering and principal component analysis both to reduce the dimension of the problem.\n",
      "- **universal requirements and caveats**\n",
      "  + requirement: all variables are numeric, and not missing. Can blow out categories to be '1' or '0' for each one.\n",
      "  + caveat: normalize data (e.g. all 'inches' not some 'feet') to avoid misweighting variables with large values\n",
      "- **hierarchical clustering**<br/>\n",
      "  + algorithm: at each step join the two elements with the shortest distance between them, until there is only one element\n",
      "  + when: data exploration and as a reality check to kmeans; this is harder to apply to new observations not in the\n",
      "    original dataset but can give a reality check to identify \n",
      "- **kmeans clustering**<br/>\n",
      "  + algorithm: randomly generate 'k' centers, then move them around until the total distance of every point to its\n",
      "  nearest center is minimized\n",
      "  + when: if you want an easily explainable way to group observations. Clusters can also then become\n",
      "    inputs to a regression\n",
      "  + caveat: seed the training with specific points if you want repeatable results\n",
      "- **principal component analysis**<br/>\n",
      "  + algorithm: consider columns to be axes, and rotate these axes so that the first component is along the direction\n",
      "  of the highest variance, the second along the direction of the next highest variance, etc.\n",
      "  + when: extremely large numbers of dimensions make all distances very large and reduce the usefulness of\n",
      "    the clustering method, but PCA is still good. I and senior colleagues have done clustering with up to 1000 columns,\n",
      "    so I mean extremely large numbers of dimensions. PCA is harder to explain to people, but is good for putting back\n",
      "    into a regression. If you can just say you used PCA to identify the 5 most important components to use in a regression\n",
      "    without having to explain what PCA is, that's good."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.spatial.distance as dist\n",
      "import scipy.cluster.hierarchy as hclust"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chicago_returns\n",
      "# chicago_returns.dropna(subset=[y, 'sp500'])[[y, 'sp500']]\n",
      "# Spatial distance: scipy.spatial.distance.pdist(X)\n",
      "#                   http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\n",
      "# Perform the hierarchical clustering: scipy.cluster.hierarchy.linkage(distance_matrix)\n",
      "#                   https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
      "# Plot the dendrogram:  den = hclust.dendrogram(links,labels=chicago_dist.columns) #, orientation=\"left\")\n",
      "#\n",
      "#\n",
      "\n",
      "chicago_dist = dist.pdist(chicago_returns.dropna().transpose(), 'euclidean')\n",
      "links = hclust.linkage(chicago_dist)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chicago_dist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(5,10))\n",
      "#data_link = hclust.linkage(chicago_dist, method='single', metric='euclidean')\n",
      "\n",
      "den = hclust.dendrogram(links,labels=chicago_returns.columns, orientation=\"left\")\n",
      "\n",
      "plt.ylabel('Samples', fontsize=9)\n",
      "plt.xlabel('Distance')\n",
      "plt.suptitle('Stocks clustered by similarity', fontweight='bold', fontsize=14);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"sp500_columns.txt\") as infile:\n",
      "    sp_companies = infile.read().strip().split(\"\\n\")\n",
      "\n",
      "returns = db.select( ('SELECT dt, \"{}\" FROM return '\n",
      "                      'WHERE dt BETWEEN \\'2012-01-01\\' AND \\'2012-12-31\\''\n",
      "                      'ORDER BY dt;').format(\n",
      "                             '\", \"'.join((c.lower() for c in sp_companies))),\n",
      "                            columns=[\"Date\"] + sp_companies)\n",
      "\n",
      "sp_dates = [row.pop(\"Date\") for row in returns]\n",
      "returns = pd.DataFrame(returns, index=sp_dates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Calculate distance and cluster\n",
      "sp_dist = dist.pdist(returns.dropna().transpose(), 'euclidean')\n",
      "sp_links = hclust.linkage(sp_dist, method='single', metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10,180))\n",
      "#data_link = hclust.linkage(chicago_dist, method='single', metric='euclidean')\n",
      "\n",
      "den = hclust.dendrogram(sp_links,labels=returns.columns, orientation=\"left\")\n",
      "\n",
      "plt.ylabel('Samples', fontsize=9)\n",
      "plt.xlabel('Distance')\n",
      "plt.suptitle('Stocks clustered by similarity', fontweight='bold', fontsize=14)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Perform Kmean clustering using scipy Kmeans"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "returns.transpose().dropna().shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.cluster.vq import  whiten\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(n_components=2)\n",
      "pca.fit(returns.transpose().dropna())\n",
      "princ_comp_returns = pca.transform(returns.transpose().dropna())\n",
      "princ_comp_returns.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "normalize =  whiten(returns.transpose().dropna())\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(normalize)\n",
      "centroids = km.cluster_centers_\n",
      "#idx = vq(normalize, centriods)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(n_components=2)\n",
      "pca.fit(returns.transpose().dropna())\n",
      "princ_comp = pca.transform(centroids)\n",
      "princ_comp\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colors = km.labels_\n",
      "plt.scatter(princ_comp_returns[:,0], princ_comp_returns[:, 1], c= colors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##8 weeks left; goals\n",
      "- blog style (so look into the free MongoDB account? Compose (formerly Mongo HQ) | MongoLabs)\n",
      "- Blog topics\n",
      "   - MPT\n",
      "   - zipline on some simple trading strategy\n",
      "   - data exploration (maybe dropdown with of the plot we already did)\n",
      "   - Belgian prof's strategy?\n",
      "   - twitter sentiment analysis to stock picks?<br/>\n",
      "     (involves dropbox and reading external file on quantopian)\n",
      "   - Machine learning\n",
      "      - natural clustering vs NAICS (?)\n",
      "      - some sort of classification 'buy' 'sell' 'ignore'(?) on stocks\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##MPT\n",
      "\n",
      "The Modern Portfolio Theory code selects an 'efficient frontier' of the minimum risk for any desired return by using linear combinations of available stocks. Stocks should be grouped into categories if there is a risk of very high correlation between pairs of stocks to reduce the risk of inverting a singular matrix.\n",
      "\n",
      "###Portfolio return\n",
      "The return of a portfolio is equal to the sum of the individual returns of the stocks multiplied by their percent allocations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{aligned}\n",
      "r_{portfolio} =\n",
      "  \\left(\\begin{matrix}p_1 & p_2 & \\ldots & p_n \\end{matrix}\\right) \\, \\cdot \\,\n",
      "    \\left(\\begin{matrix}r_1\\\\ r_2 \\\\ \\vdots \\\\ r_n \\end{matrix}\\right)\n",
      "\\end{aligned}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Portfolio risk\n",
      "The risk (variance) of a portfolio is the variance of a sum of random variables (the individual stock portfolios, multiplied by percent allocation). And the equation for variance of a sum of random variables is:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{aligned}\n",
      "sd( x_1 + x_2) = sd(x_1) + sd(x_2) - 2 \\, \\cdot \\, cov(x_1, x_2) \\, \\cdot \\, sd(x_1) \\, \\cdot \\, sd(x_2)\n",
      "\\\\[0.25in]\n",
      "sd( portfolio ) =\n",
      "  \\left(\\begin{matrix}p_1 & p_2 & \\ldots & p_n \\end{matrix}\\right) \\, \\cdot \\,\n",
      "    \\left(\\begin{matrix}cov(x_1,x_1) & cov(x_1, x_2) &\\ldots & cov(x_1, x_n)\\\\\n",
      "                        cov(x_2, x1) & cov(x_2, x_2) &\\ldots & cov(x_2, x_n)  \\\\\n",
      "                        \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
      "                        cov(x_n, x_1)& cov(x_n, x_2) & \\ldots & cov(x_n,x_n) \\end{matrix}\\right)\n",
      "    \\left(\\begin{matrix}p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_n \\end{matrix}\\right)\n",
      "\\end{aligned}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mpt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date_range = {\"start\": datetime.date(2009,1,1), \"end\": datetime.date(2012,12,31)}\n",
      "\n",
      "chicago_companies = ['ADM',  'BA', 'MDLZ', 'ALL', 'MCD', 'EXC', 'ABT',\n",
      "                     'KRFT', 'ITW', 'GWW',  'DFS', 'DOV', 'MSI',\n",
      "                     'NI', 'TEG', 'CF' ]\n",
      "chicago_returns = db.select(('SELECT dt, \"{}\" FROM return'\n",
      "                             ' WHERE dt BETWEEN ''%s'' AND ''%s'''\n",
      "                             ' ORDER BY dt;').format(\n",
      "                             '\", \"'.join((c.lower() for c in chicago_companies))),\n",
      "                            columns=[\"Date\"] + chicago_companies,\n",
      "                            args=[date_range['start'], date_range['end']])\n",
      "\n",
      "# At this point chicago_returns is an array of dictionaries\n",
      "# [{\"Date\":datetime.date(2009,1,1), \"ADM\":0.1, \"BA\":0.2, etc ... , \"CF\": 0.1},\n",
      "#  {\"Date\":datetime.date(2009,1,2), \"ADM\":0.2, \"BA\":0.1, etc ..., \"CF\":0.1},\n",
      "#  ...,\n",
      "#  {\"Date\":datetime.date(2012,12,31), \"ADM\":0.2, \"BA\":0.1, etc ..., \"CF\":0.1}]\n",
      "\n",
      "\n",
      "# Pull out the dates to mak them the indices in the Pandas DataFrame\n",
      "chi_dates = [row.pop(\"Date\") for row in chicago_returns]\n",
      "chicago_returns = pd.DataFrame(chicago_returns, index=chi_dates)\n",
      "chicago_returns = chicago_returns / 100\n",
      "\n",
      "treas90 = web.DataReader('^IRX', 'yahoo', date_range['start'], date_range['end'])\n",
      "treas90['treas90'] = treas90['Adj Close'].diff() / treas90['Adj Close']\n",
      "\n",
      "chicago_returns = chicago_returns.join(treas90['treas90'])\n",
      "chicago_returns.drop(chicago_returns.head(1).index, inplace=True)\n",
      "\n",
      "chicago_returns.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "result = mpt.get_efficient_frontier(chicago_returns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chicago_returns.std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "fig.suptitle(\"MPT Efficient Frontier\", fontsize=14, fontweight='bold')\n",
      "\n",
      "ax = fig.add_subplot(111)\n",
      "fig.subplots_adjust(top=0.85)\n",
      "\n",
      "ax.set_xlabel('Risk')\n",
      "ax.set_ylabel('Target return (annual)')\n",
      "\n",
      "sds = list(chicago_returns.std()*np.sqrt(250))\n",
      "mus = list(chicago_returns.mean()*250)\n",
      "syms = list(chicago_returns.columns)\n",
      "\n",
      "#for i in range(len(sds)):\n",
      "#  plt.text(sds[i], mus[i], syms[i], bbox={'facecolor':'slateblue', 'alpha':0.5, 'pad':10})\n",
      "\n",
      "    \n",
      "ax.plot(result[\"risks\"], result[\"returns\"], linewidth=2)\n",
      "ax.scatter(sds, mus, alpha=0.5)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": false
     },
     "source": [
      "#Plotting with plot.ly\n",
      "I could not use pip's install of plotly's Python api because it was missing almost all of the submodules (at least for Python 2.7). Instead, install from github:\n",
      "\n",
      "    git clone https://github.com/plotly/python-api.git\n",
      "    cd python-api\n",
      "    python setup.py install\n",
      "    \n",
      "And also configure plotly for your account, per the [instructions here] [plotly_quickstart]. My configuration:\n",
      "\n",
      "    python -c \"import plotly; plotly.tools.set_credentials_file(username='tanya.schlusser', api_key='9fjjjaguyh', stream_ids=['n8gn5rg5ag', 'dlsytpqrr0'])\"\n",
      "    \n",
      "    \n",
      "[plotly_quickstart]: https://plot.ly/python/getting-started/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}